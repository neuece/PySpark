{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loads necessary supporting packages and creates HiveContext for loading Hive tables into memory\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col, array, avg, approx_count_distinct, countDistinct\n",
    "from pyspark.ml.feature import QuantileDiscretizer, Bucketizer, Imputer\n",
    "from pyspark.sql import functions as f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.window import Window\n",
    "import re\n",
    "\n",
    "\n",
    "# Creates instance of HiveContext necessary for interacting with Hive\n",
    "hive_context = HiveContext(sc)\n",
    "\n",
    "# Creates the class needed to return error messages when pre-requisites are not met\n",
    "class HaltException(Exception): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data of Date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read selected features in previous step\n",
    "\n",
    "select_med_list=pd.read_csv('PROD_MED_FS_COLUMNS_FINAL_T.csv')\n",
    "select_med_list=select_med_list.name.tolist()\n",
    "\n",
    "select_rx_list=pd.read_csv('PROD_RX_FS_COLUMNS_FINAL_T.csv')\n",
    "select_rx_list=select_rx_list.name.tolist()\n",
    "\n",
    "select_col_list = select_med_list+select_rx_list\n",
    "select_col_list.append('tgt_cost')\n",
    "select_col_list.append('member_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare data from different source\n",
    "\n",
    "temp_df1  = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM medfeats1_data1 \n",
    "   \n",
    "   \"\"\"\n",
    "    )\n",
    "\n",
    "temp_df2  = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM medfeats2_data1 \n",
    "   \n",
    "   \"\"\"\n",
    "    ).drop('tgt_cost')\n",
    "\n",
    "temp_df3 = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM rxfeats_data1 \n",
    "   \n",
    "   \"\"\"\n",
    "    ).drop('tgt_cost','pgk_rx')\n",
    "\n",
    "mbr_df  = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM mbrlist_data1\n",
    "   \n",
    "   \"\"\"\n",
    "    )\n",
    "\n",
    "temp_df = (temp_df1.join(temp_df2,'member_key').join(temp_df3,'member_key')\n",
    "           .select([temp_df1[\"*\"]]+[c for c in temp_df2.columns if c not in ['member_key']]\n",
    "                  +[c for c in temp_df3.columns if c not in ['member_key']]))\n",
    "\n",
    "temp_df = (temp_df.select(select_col_list).withColumn(\"Process_Month\",f.lit(\"data1\"))\n",
    "           .withColumn(\"raw_tgt\",temp_df.tgt_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# log/sqrt transform for pmpm type variables to make the distribution Gaussian\n",
    "pmpm_column_list = [col for col in temp_df.columns if ((('pmpm_' in col) | ('PMPM_' in col)) & (('_cost' in col) | ('_COST' in col)))]\n",
    "pmpm_column_list.append('tgt_cost')\n",
    "pmpm_cnt_list = [col for col in temp_df.columns if ((('pmpm_' in col) | ('PMPM_' in col)) & (col not in pmpm_column_list))]\n",
    "pmpm_df = temp_df.select(['member_key']+[f.log(c).alias(c) for c in pmpm_column_list]+[f.sqrt(c).alias(c) for c in pmpm_cnt_list])\n",
    "pmpm_df = pmpm_df.na.fill(-4.60517)\n",
    "pmpm_column_list = pmpm_column_list + pmpm_cnt_list\n",
    "print(len(pmpm_column_list))\n",
    "temp_df=temp_df.drop(*pmpm_column_list)\n",
    "print len(temp_df.columns)\n",
    "temp_df = temp_df.join(pmpm_df,'member_key').select([temp_df[\"*\"]]+[c for c in pmpm_df.columns if c not in ['member_key']])\n",
    "print len(temp_df.columns)\n",
    "temp_df = (temp_df.join(mbr_df,'member_key')\n",
    "           .select(temp_df[\"*\"],mbr_df[\"age\"])\n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data of Date2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df1  = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM medfeats1_Data2 \n",
    "   \n",
    "   \"\"\"\n",
    "    )\n",
    "\n",
    "temp_df2  = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM medfeats2_Data2 \n",
    "   \n",
    "   \"\"\"\n",
    "    ).drop('MEDICARE_ID','tgt_cost')\n",
    "\n",
    "temp_df3 = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM rxfeats_Data2 \n",
    "   \n",
    "   \"\"\"\n",
    "    ).drop('MEDICARE_ID','tgt_cost','pgk_rx')\n",
    "\n",
    "mbr_df  = hive_context.sql(\n",
    "    \"\"\"SELECT * \n",
    "    FROM mbrlist_Data2\n",
    "   \n",
    "   \"\"\"\n",
    "    )\n",
    "\n",
    "temp_df_02 = (temp_df1.join(temp_df2,'member_key').join(temp_df3,'member_key')\n",
    "           .select([temp_df1[\"*\"]]+[c for c in temp_df2.columns if c not in ['member_key']]\n",
    "                  +[c for c in temp_df3.columns if c not in ['member_key']]))\n",
    "\n",
    "temp_df_02 = (temp_df_02.select(select_col_list).withColumn(\"Process_Month\",f.lit(\"Data2\"))\n",
    "              .withColumn(\"raw_tgt\",temp_df_02.tgt_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# log/sqrt transform for pmpm type variables to make the distribution Gaussian\n",
    "\n",
    "pmpm_column_list = [col for col in temp_df_02.columns if ((('pmpm_' in col) | ('PMPM_' in col)) & (('_cost' in col) | ('_COST' in col)))]\n",
    "pmpm_column_list.append('tgt_cost')\n",
    "pmpm_cnt_list = [col for col in temp_df_02.columns if ((('pmpm_' in col) | ('PMPM_' in col)) & (col not in pmpm_column_list))]\n",
    "pmpm_df = temp_df_02.select(['member_key']+[f.log(c).alias(c) for c in pmpm_column_list]+[f.sqrt(c).alias(c) for c in pmpm_cnt_list])\n",
    "pmpm_df = pmpm_df.na.fill(-4.60517)\n",
    "pmpm_column_list = pmpm_column_list + pmpm_cnt_list\n",
    "print(len(pmpm_column_list))\n",
    "temp_df_02=temp_df_02.drop(*pmpm_column_list)\n",
    "print len(temp_df_02.columns)\n",
    "temp_df_02 = temp_df_02.join(pmpm_df,'member_key').select([temp_df_02[\"*\"]]+[c for c in pmpm_df.columns if c not in ['member_key']])\n",
    "print len(temp_df_02.columns)\n",
    "temp_df_02 = (temp_df_02.join(mbr_df,'member_key')\n",
    "           .select(temp_df_02[\"*\"],mbr_df[\"age\"])\n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_02_mbr_list = temp_df_02.sample(False, 0.6, seed=123).select('member_key').withColumnRenamed('member_key','pgk1')\n",
    "test_02_mbr_list = temp_df_02.select('member_key').join(train_02_mbr_list,temp_df_02.member_key==train_02_mbr_list.pgk1,'left')\n",
    "test_02_mbr_list = test_02_mbr_list.filter(f.col('pgk1').isNull()).drop(\"pgk1\")\n",
    "\n",
    "test_04_mbr_list = (temp_df.select('member_key').join(test_02_mbr_list,temp_df.member_key==test_02_mbr_list.member_key)\n",
    "                    .select(temp_df.member_key).withColumnRenamed('member_key','pgk1'))\n",
    "train_04_mbr_list = temp_df.select('member_key').join(test_04_mbr_list,temp_df.member_key==test_04_mbr_list.pgk1,'left')\n",
    "train_04_mbr_list = train_04_mbr_list.filter(f.col('pgk1').isNull())\n",
    "# print(test_04_mbr_list.count())\n",
    "# print(train_04_mbr_list.count())\n",
    "# print(temp_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_02_temp = temp_df_02.join(train_02_mbr_list,temp_df_02.member_key == train_02_mbr_list.pgk1).select(temp_df_02[\"*\"])\n",
    "train_04_temp = temp_df.join(train_04_mbr_list,'member_key').select(temp_df[\"*\"])\n",
    "\n",
    "test_02_temp = temp_df_02.join(test_02_mbr_list,'member_key').select(temp_df_02[\"*\"])\n",
    "test_04_temp = temp_df.join(test_04_mbr_list,temp_df.member_key == test_04_mbr_list.pgk1).select(temp_df[\"*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_temp_id_df = train_02_temp.union(train_04_temp)\n",
    "train_temp_id_df = train_temp_id_df.na.fill({'RACE_CD':'Unknown'})\n",
    "test_temp_id_df = test_02_temp.union(test_04_temp)\n",
    "test_temp_id_df = test_temp_id_df.na.fill({'RACE_CD':'Unknown'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a ML pipeline to build different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler,QuantileDiscretizer,StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Character Features\n",
    "char_df_final=(train_temp_id_df.select(\"RACE_CD\",\"member_key\"))\n",
    "           \n",
    "# Numeric Features\n",
    "num_df_final=train_temp_id_df.select([item[0] for item in train_temp_id_df.dtypes if (item[1] != 'string')])\n",
    "\n",
    "numlist=([c for c in num_df_final.columns if c not in [\"member_key\",\"raw_tgt\",\"Process_Month\"] ])\n",
    "charlist=([c for c in char_df_final.columns if c not in [\"member_key\",\"Process_Month\"]]) \n",
    "\n",
    "string_feature_indexers = [StringIndexer(inputCol=x, outputCol=\"intx_{0}\".format(x)) for x in charlist]\n",
    "\n",
    "train_final_df = Pipeline(stages=string_feature_indexers).fit(train_temp_id_df).transform(train_temp_id_df) \n",
    "test_final_df = Pipeline(stages=string_feature_indexers).fit(train_temp_id_df).transform(test_temp_id_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelname='tgt_cost'\n",
    "numlist.remove(labelname)\n",
    "all_columns = numlist + [\"intx_\"+x for x in charlist]\n",
    "\n",
    "# Assembler\n",
    "assembler = VectorAssembler(inputCols=[col for col in all_columns], outputCol=\"features\")\n",
    "\n",
    "#Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# define evaluator with different metric\n",
    "evaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"tgt_cost\",metricName=\"r2\")\n",
    "evaluator_mse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"tgt_cost\",metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract feature importance from RF. not used here.\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "# df_med_fs=ExtractFeatureImp(model_fs.stages[-1].featureImportances, prediction_fs, \"features\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter is not fine tuned because we are going to ensemle all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(labelCol=\"tgt_cost\", featuresCol=\"scaled_features\", numTrees=20, maxDepth=16, maxBins = 20)\n",
    "pipeline_rf = Pipeline(stages=[assembler, scaler, rf])\n",
    "model_rf = pipeline_rf.fit(train_final_df)\n",
    "prediction_rf=model_rf.transform(test_final_df)\n",
    "train_pre_rf = model_rf.transform(train_final_df)\n",
    "# train_pre_rf = model_rf.transform(train_temp_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_rf))\n",
    "# print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_rf))\n",
    "\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_rf))\n",
    "print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_rf))\n",
    "print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_rf))\n",
    "print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf2 = RandomForestRegressor(labelCol=\"tgt_cost\", featuresCol=\"features\", numTrees=50, maxDepth=16, maxBins = 10)\n",
    "pipeline_rf2 = Pipeline(stages=[assembler, scaler, rf2])\n",
    "model_rf2 = pipeline_rf2.fit(train_final_df)\n",
    "prediction_rf2 =model_rf2.transform(test_final_df)\n",
    "train_pre_rf2 = model_rf2.transform(train_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_rf2))\n",
    "print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_rf2))\n",
    "\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_rf2))\n",
    "print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rf2 = RandomForestRegressor(labelCol=\"tgt_cost\", featuresCol=\"features\", numTrees=100, maxDepth=16, maxBins = 20)\n",
    "# pipeline_rf2 = Pipeline(stages=[assembler, scaler, rf2])\n",
    "# model_rf2 = pipeline_rf2.fit(train_final_df)\n",
    "# prediction_rf2 =model_rf2.transform(test_final_df)\n",
    "# train_pre_rf2 = model_rf2.transform(train_final_df)\n",
    "# print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_rf2))\n",
    "# print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_rf2))\n",
    "\n",
    "# print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_rf2))\n",
    "# print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_rf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol=\"tgt_cost\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
    "model_lr = pipeline_lr.fit(train_final_df)\n",
    "prediction_lr = model_lr.transform(test_final_df)\n",
    "train_pre_lr = model_lr.transform(train_final_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_lr))\n",
    "print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_lr))\n",
    "print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_lr))\n",
    "print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr2 = LinearRegression(featuresCol = 'features', labelCol=\"tgt_cost\", maxIter=20, regParam=0.01)\n",
    "pipeline_lr2 = Pipeline(stages=[assembler, scaler, lr2])\n",
    "model_lr2 = pipeline_lr2.fit(train_final_df)\n",
    "prediction_lr2 = model_lr2.transform(test_final_df)\n",
    "train_pre_lr2 = model_lr2.transform(train_final_df)\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_lr2))\n",
    "print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_lr2))\n",
    "print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_lr2))\n",
    "print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: DTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = \"tgt_cost\",maxBins=10,maxDepth=10)\n",
    "pipeline_dt = Pipeline(stages=[assembler, scaler, dt])\n",
    "model_dt = pipeline_dt.fit(train_final_df)\n",
    "prediction_dt = model_dt.transform(test_final_df)\n",
    "train_pre_dt = model_dt.transform(train_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_dt))\n",
    "print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_dt))\n",
    "print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_dt))\n",
    "print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = \"tgt_cost\", maxIter=20, maxDepth=10)\n",
    "pipeline_gbt = Pipeline(stages=[assembler, scaler, gbt])\n",
    "model_gbt = pipeline_gbt.fit(train_final_df)\n",
    "prediction_gbt = model_gbt.transform(test_final_df)\n",
    "train_pre_gbt = model_gbt.transform(train_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_gbt))\n",
    "print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_gbt))\n",
    "print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_gbt))\n",
    "print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1 = train_pre_rf.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('prediction','p1')\n",
    "m2 = train_pre_rf2.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK2').withColumnRenamed('prediction','p2').withColumnRenamed('Process_Month','PM2')\n",
    "m3 = train_pre_lr.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK3').withColumnRenamed('prediction','p3').withColumnRenamed('Process_Month','PM3')\n",
    "m4 = train_pre_dt.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK4').withColumnRenamed('prediction','p4').withColumnRenamed('Process_Month','PM4')\n",
    "m5 = train_pre_gbt.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK5').withColumnRenamed('prediction','p5').withColumnRenamed('Process_Month','PM5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1t = prediction_rf.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('prediction','p1').withColumnRenamed('member_key','PGK1')\n",
    "m2t = prediction_rf2.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK2').withColumnRenamed('prediction','p2').withColumnRenamed('Process_Month','PM2')\n",
    "m3t = prediction_lr.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK3').withColumnRenamed('prediction','p3').withColumnRenamed('Process_Month','PM3')\n",
    "m4t = prediction_dt.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK4').withColumnRenamed('prediction','p4').withColumnRenamed('Process_Month','PM4')\n",
    "m5t = prediction_gbt.select('prediction', \"tgt_cost\",'member_key','Process_Month').withColumnRenamed('member_key','PGK5').withColumnRenamed('prediction','p5').withColumnRenamed('Process_Month','PM5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_all_df = (m1.join(m2,(m1.member_key==m2.PGK2) & (m1.Process_Month == m2.PM2))\n",
    "                     .join(m3,(m1.member_key==m3.PGK3) & (m1.Process_Month == m3.PM3))\n",
    "                     .join(m4,(m1.member_key==m4.PGK4) & (m1.Process_Month == m4.PM4))\n",
    "                     .join(m5,(m1.member_key==m5.PGK5) & (m1.Process_Month == m5.PM5))\n",
    "                     .select(m1[\"*\"],m2.p2, m3.p3, m4.p4, m5.p5)\n",
    "                    )\n",
    "\n",
    "\n",
    "prediction_all_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check point save\n",
    "\n",
    "# hive_context.sql('DROP TABLE IF EXISTS ens_df_1')\n",
    "# (m1t.write.option(\"path\", \"/user/hive/warehouse/db/ens_df_1\")\n",
    "#        .saveAsTable(\"ens_df_1\"))\n",
    "# hive_context.sql('DROP TABLE IF EXISTS ens_df_2')\n",
    "# (m2t.write.option(\"path\", \"/user/hive/warehouse/db/ens_df_2\")\n",
    "#        .saveAsTable(\"ens_df_2\"))\n",
    "# hive_context.sql('DROP TABLE IF EXISTS ens_df_3')\n",
    "# (m3t.write.option(\"path\", \"/user/hive/warehouse/db/ens_df_3\")\n",
    "#        .saveAsTable(\"ens_df_3\"))\n",
    "# hive_context.sql('DROP TABLE IF EXISTS ens_df_4')\n",
    "# (m4t.write.option(\"path\", \"/user/hive/warehouse/db/ens_df_4\")\n",
    "#        .saveAsTable(\"ens_df_4\"))\n",
    "# hive_context.sql('DROP TABLE IF EXISTS ens_df_5')\n",
    "# (m5t.write.option(\"path\", \"/user/hive/warehouse/db/ens_df_5\")\n",
    "#        .saveAsTable(\"ens_df_5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load check point\n",
    "\n",
    "# m1t_r  = hive_context.sql(\n",
    "#     \"\"\"SELECT * \n",
    "#     FROM ens_df_1\n",
    "   \n",
    "#    \"\"\")\n",
    "\n",
    "# m2t_r  = hive_context.sql(\n",
    "#     \"\"\"SELECT * \n",
    "#     FROM ens_df_2\n",
    "   \n",
    "#    \"\"\")\n",
    "\n",
    "# m3t_r  = hive_context.sql(\n",
    "#     \"\"\"SELECT * \n",
    "#     FROM ens_df_3\n",
    "   \n",
    "#    \"\"\")\n",
    "\n",
    "# m4t_r  = hive_context.sql(\n",
    "#     \"\"\"SELECT * \n",
    "#     FROM ens_df_4\n",
    "   \n",
    "#    \"\"\")\n",
    "\n",
    "# m5t_r  = hive_context.sql(\n",
    "#     \"\"\"SELECT * \n",
    "#     FROM ens_df_5\n",
    "   \n",
    "#    \"\"\")\n",
    "\n",
    "\n",
    "# testing_all_df = (m1t_r.join(m2t_r,(m1t_r.PGK1==m2t_r.PGK2) & (m1t_r.Process_Month == m2t_r.PM2),'left')\n",
    "#                      .join(m3t_r,(m1t_r.PGK1==m3t_r.PGK3) & (m1t_r.Process_Month == m3t_r.PM3))\n",
    "#                      .join(m4t_r,(m1t_r.PGK1==m4t_r.PGK4) & (m1t_r.Process_Month == m4t_r.PM4))\n",
    "#                      .join(m5t_r,(m1t_r.PGK1==m5t_r.PGK5) & (m1t_r.Process_Month == m5t_r.PM5))\n",
    "#                      .select(m1t_r[\"*\"],m2t_r.p2, m3t_r.p3, m4t_r.p4, m5t_r.p5)\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['p1', 'p2', 'p3', 'p4','p5'], outputCol = 'features_en')\n",
    "prediction_all_vdf = vectorAssembler.transform(prediction_all_df)\n",
    "prediction_all_vdf = prediction_all_vdf.select(['features_en', \"tgt_cost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LREN = LinearRegression(featuresCol = 'features_en', labelCol=\"tgt_cost\",maxIter=25)\n",
    "LREN_model = LREN.fit(prediction_all_vdf)\n",
    "train_pre_ensmb = LREN_model.transform(prediction_all_vdf)\n",
    "print(\"Coefficients: \" + str(LREN_model.coefficients))\n",
    "print(\"Intercept: \" + str(LREN_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_all_vdf = vectorAssembler.transform(testing_all_df)\n",
    "testing_all_vdf = testing_all_vdf.select(['features_en', \"tgt_cost\",'PGK1', 'Process_Month'])\n",
    "prediction_ensmb = LREN_model.transform(testing_all_vdf)\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator_r2.evaluate(prediction_ensmb))\n",
    "print(\"MSE (rmse) on test data = %g\" % evaluator_mse.evaluate(prediction_ensmb))\n",
    "print(\"R Squared (R2) on training data = %g\" % evaluator_r2.evaluate(train_pre_ensmb))\n",
    "print(\"MSE (rmse) on training data = %g\" % evaluator_mse.evaluate(train_pre_ensmb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put members into different severity bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stat_df = prediction_ensmb.select('tgt_cost', 'PGK1', 'Process_Month', 'prediction').toPandas()\n",
    "\n",
    "stat_df = prediction_rf2.select('tgt_cost', 'member_key', 'Process_Month', 'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stat_df['rank_actual']=stat_df['tgt_cost'].rank(ascending=False)\n",
    "stat_df['perc_actual']=stat_df['rank_actual']/stat_df.shape[0]\n",
    "stat_df['rank_pred']=stat_df['prediction'].rank(ascending=False)\n",
    "stat_df['perc_pred']=stat_df['rank_pred']/stat_df.shape[0]\n",
    "def bucket_customized(p,c1,c2,c3):\n",
    "    \n",
    "    if p<=c1:\n",
    "        cat='High'\n",
    "    elif (p>c1) & (p<=c2):\n",
    "        cat = 'Med'\n",
    "    elif (p>c2) & (p<=c3):\n",
    "        cat = 'Low' \n",
    "    else: cat = 'MNT'\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stat_df['perct_act_cat'] = stat_df.apply(lambda row: bucket_customized(row['perc_actual'],0.2,0.4,0.6), axis=1)\n",
    "stat_df['perct_pre_cat'] = stat_df.apply(lambda row: bucket_customized(row['perc_pred'],0.2,0.4,0.6), axis=1)\n",
    "\n",
    "stat_df['perct_act_cat'] = stat_df.apply(lambda row: bucket_customized(row['perc_actual'],0.15,0.3,0.45), axis=1)\n",
    "stat_df['perct_pre_cat'] = stat_df.apply(lambda row: bucket_customized(row['perc_pred'],0.15,0.3,0.45), axis=1)\n",
    "# stat_df['perct_act_cat'] = stat_df.apply(lambda row: bucket_customized(row['perc_actual'],0.9,0.7,0.5), axis=1)\n",
    "# stat_df['perct_pre_cat'] = stat_df.apply(lambda row: bucket_customized(row['perc_pred'],0.9,0.7,0.5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stat_df.groupby([\"perct_act_cat\", \"perct_pre_cat\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stat_df = (stat_df.join(test_final_df, (stat_df.PGK1==test_final_df.member_key) & (stat_df.Process_Month==test_final_df.Process_Month))\n",
    "           .select(stat_df[\"*\"],test_final_df. , test_final_df. , test_final_df,)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaldf_test = prediction_rf2.select(['tgt_cost','prediction'])\n",
    "edf_list = [(r.tgt_cost,r.prediction) for r in evaldf_test.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = [x[0] for x in edf_list]\n",
    "predicted = [x[1] for x in edf_list]\n",
    "act_pred = [actual,predicted]\n",
    "labels = ['actual','predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(act_pred)): \n",
    "    # Draw the density plot\n",
    "    sns.distplot(act_pred[i], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = labels[i])\n",
    "    \n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.title('Density Plot of Actual and Predicted values')\n",
    "plt.xlabel('Log_total_pmpm')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaldf_test2 = prediction_rf2.select(['raw_tgt','prediction'])\n",
    "edf_list2 = [(r.raw_tgt,r.prediction) for r in evaldf_test2.collect()]\n",
    "actual2 = [x[0] for x in edf_list2]\n",
    "predicted2 = [math.exp(x[1]) for x in edf_list2]\n",
    "act_pred2 = [actual2,predicted2]\n",
    "labels = ['actual','predicted']\n",
    "plt.hist(act_pred2[0], bins=np.logspace(np.log10(0.1),np.log10(60000.0), 50),alpha=0.5)\n",
    "plt.hist(act_pred2[1], color='r',bins=np.logspace(np.log10(0.1),np.log10(60000.0), 50),alpha=0.5)\n",
    "\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_rf.save(\"rf1\") \n",
    "model_rf2.save(\"rf2\") \n",
    "model_lr.save(\"lr1\")\n",
    "model_lr2.save(\"lr2\")\n",
    "model_gbt.save(\"gbt\")\n",
    "model_dt.save(\"dt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
